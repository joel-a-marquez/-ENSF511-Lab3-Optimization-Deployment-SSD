{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e272fdd6",
   "metadata": {},
   "source": [
    "# ENSF 511, W2022, Lab 3 - Part 2 (Different Sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8be4e5",
   "metadata": {},
   "source": [
    "   ### Joel Aaron Marquez, Edited on April 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbd85c2",
   "metadata": {},
   "source": [
    "##### Part 1 - Preprocessing the data\n",
    "\n",
    "When preparing the data to train the model for SSD (single shot detector for object detection), we need to provide details of where the objects are on the image. Each image will have another file that describes the bounding boxes (xmin, ymin, xmax, ymax) for each object in the image in a **PascalVOC XML format** (figure below). This process is simplified through the **LabelImg Python Program** (link [here](https://github.com/tzutalin/labelImg)).\n",
    "\n",
    "                                Figure 1 - PascalVOC XML file for bobcat class image\n",
    "![pascal_voc_xml](../images-for-discussion/pascalvoc.jpg)\n",
    "\n",
    "The bobcat and cat images have 4000 images in total. Therefore, due to the limited time, the bounding boxes are **all constant** for all images [size (1,1,224,224)]. The code below is used to populate and create the xml files for each name and class specific for the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3bba537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the xml files for each image (created a sample bounding from LabelImg)\n",
    "# only change name of file and the filename in the annotation\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "train = \"C:/Users/User/Downloads/Lab3/MobileNet-samples/train\"\n",
    "valid = \"C:/Users/User/Downloads/Lab3/MobileNet-samples/validation\"\n",
    "path = \"C:/Users/User/Downloads/RealTimeObjectDetection-main/RealTimeObjectDetection-main/Tensorflow/workspace/images\"\n",
    "        \n",
    "folders = [train,valid]\n",
    "n_folder = ['test','train']\n",
    "\n",
    "# copying the new xml files with identical bounds (the whole image) and the image as well\n",
    "for folder,new_f in zip(folders,n_folder):\n",
    "\n",
    "    for i_class in os.listdir(folder):\n",
    "        if not os.path.exists(path + \"/\" + new_f + \"/\" + i_class):\n",
    "            os.makedirs(path + \"/\" + new_f + \"/\" + i_class)\n",
    "            \n",
    "        for image in os.listdir(folder + \"/\" + i_class):\n",
    "            new_xml = path + \"/\" + new_f + \"/\" + i_class + \"/\" + re.findall(\"(.+).jpg\",image)[0] + \".xml\"\n",
    "            \n",
    "            #editing and saving xml file\n",
    "            tree = ET.parse('C:/Users/User/Downloads/Lab3/labels/abordable-eurasian-lynx-portrait-winter-field-beautiful-bobcat-140392804.xml')\n",
    "            root = tree.getroot()\n",
    "            element = root[1]\n",
    "            element.text = image\n",
    "            element2 = root[6][0]\n",
    "            element2.text = i_class\n",
    "            tree.write(new_xml)\n",
    "            \n",
    "            #copying image\n",
    "            shutil.copy(folder + \"/\" + i_class + \"/\"  + image, path + \"/\" + new_f + \"/\" + i_class + \"/\" + image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6529d688",
   "metadata": {},
   "source": [
    "###### Part 2 - Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9530c9ec",
   "metadata": {},
   "source": [
    "The next step now is to train the model. From what I can understand, training object detection models are slightly different. I followed a Youtube [tutorial](https://www.youtube.com/watch?v=IOI0o3Cxv9Q&list=LL&index=2) and a github [repository](https://github.com/nicknochnack/RealTimeObjectDetection) to help ease the training process for the ssd_mobilenetv1 model. Unfortunately, I had issues with the training that I was running out of time that I couldn't trian a model, deploy, and test the performance on live video. The details of the process/challenges for this part will be explained in the **discussion** portion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ENSF511-Lab2)",
   "language": "python",
   "name": "ensf511-lab2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
